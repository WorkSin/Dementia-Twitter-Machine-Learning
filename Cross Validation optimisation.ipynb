{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "##Explore this blog: https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html for visualisation ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads rated tweets into dataframe\n",
    "df = pd.read_excel(\"C:/Users/ertur/Documents/Work/Workwork/ARUK/Submission - JMIR Aging/Revisions/Categorised tweets 1500.xlsx\", converters={'Tweet':str,'Theme':int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'Tweet':'body_text', 'Theme':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1497, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing cases where rating is missing\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining sentiment and subjectivity\n",
    "def sentAnal(df):\n",
    "    for index, row in df.iterrows():\n",
    "        temp = TextBlob(row['body_text'])\n",
    "        df.loc[index,'Sentiment'] = temp.sentiment.polarity\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sentAnal(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1414, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing tweets rated as uncertain or unknown\n",
    "themes=[1,2,3,4,5,6]\n",
    "df = df[df.label.isin(themes)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting assigned themes into corresponding rating of stigmatising and non-stigmatising\n",
    "theme_map = {1:0, 2:0, 3:0, 4:1, 5:1, 6:1}\n",
    "df['stig_label'] = df.label.map(theme_map)\n",
    "df = df.drop('label', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#literature defined features are generated\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "df['body_len'] = df['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "df['punct%'] = df['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "# Average Word Length. simply take the sum of the length of all the words and divide it by the total length of the tweet as defined in function above\n",
    "df['avg_word'] = df['body_text'].apply(lambda x: avg_word(x))\n",
    "\n",
    "# Number of Words in tweet\n",
    "df['word_count'] = df['body_text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "# Number of characters. Here, we calculate the number of characters in each tweet. This is done by calculating the length of the tweet.\n",
    "df['char_count'] = df['body_text'].str.len() ## this also includes spaces\n",
    "\n",
    "# number of special characters like hashtags. we make use of the ‘starts with’ function because hashtags (or mentions) always appear at the beginning of a word.\n",
    "df['hastags'] = df['body_text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "\n",
    "# number of numerics in tweet\n",
    "df['numerics'] = df['body_text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#care-partner defined features are generated\n",
    "#senile\n",
    "Search_for_These_values = ['senile', 'SENILE'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['senile'] = df['body_text'].str.contains(pattern)\n",
    "df['senile'] = df['senile'].map({True: 1, False: 0})\n",
    "#demented\n",
    "Search_for_These_values = ['demented', 'DEMENTED'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['demented'] = df['body_text'].str.contains(pattern)\n",
    "df['demented'] = df['demented'].map({True: 1, False: 0})\n",
    "#donald trump\n",
    "Search_for_These_values = ['donald', 'trump', 'DONALD', 'TRUMP', '@realDonaldTrump'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['donaldtrump'] = df['body_text'].str.contains(pattern)\n",
    "df['donaldtrump'] = df['donaldtrump'].map({True: 1, False: 0})\n",
    "#memory\n",
    "Search_for_These_values = ['MEMORY', 'memory'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Memory'] = df['body_text'].str.contains(pattern)\n",
    "df['Memory'] = df['Memory'].map({True: 1, False: 0})\n",
    "#research\n",
    "Search_for_These_values = ['research', 'RESEARCH'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Research'] = df['body_text'].str.contains(pattern)\n",
    "df['Research'] = df['Research'].map({True: 1, False: 0})\n",
    "#crazy\n",
    "Search_for_These_values = ['crazy', 'CRAZY'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Crazy'] = df['body_text'].str.contains(pattern)\n",
    "df['Crazy'] = df['Crazy'].map({True: 1, False: 0})\n",
    "#senility\n",
    "Search_for_These_values = ['senility', 'SENILITY'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Senility'] = df['body_text'].str.contains(pattern)\n",
    "df['Senility'] = df['Senility'].map({True: 1, False: 0})\n",
    "# URL\n",
    "Search_for_These_values = ['https'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Link'] = df['body_text'].str.contains(pattern)\n",
    "df['Link'] = df['Link'].map({True: 1, False: 0})\n",
    "#caregiver\n",
    "Search_for_These_values = ['caregiver', 'CAREGIVER'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Caregiver'] = df['body_text'].str.contains(pattern)\n",
    "df['Caregiver'] = df['Caregiver'].map({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1414, 19)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df[df.columns.difference([\"stig_label\"])].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[cols], df['stig_label'], test_size=0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df[df.columns.difference([\"stig_label\", \"body_text\"])].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of each training and testing datasets are:\n",
      "(1131, 18)\n",
      "(283, 18)\n",
      "(1131,)\n",
      "(283,)\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of each training and testing datasets are:\")\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the vectorizer\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "tfidf_vect_fit = tfidf_vect.fit(X_train['body_text'])\n",
    "tfidf_train = tfidf_vect_fit.transform(X_train['body_text'])\n",
    "\n",
    "\n",
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "tfidf_test = tfidf_vect_fit.transform(X_test['body_text'])\n",
    "\n",
    "X_train_vect = pd.concat([X_train[cols].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_train.toarray(), columns=tfidf_vect.get_feature_names())], axis=1)\n",
    "\n",
    "X_test_vect = pd.concat([X_test[cols].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_test.toarray(), columns=tfidf_vect.get_feature_names())], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Fold Validation Code from Nick**, it's slower but everything is comparable and the test vectors have been vectorised by the training vectors. I've not included all modesl, but you should be able to see the pattern of how to do it if you want to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_folds = 5 # number of folds to do being set\n",
    "kf_Strat = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=0) #sets this as a 'train test split equiv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original estimators [10, 25, 50, 100, 250, 500]\n",
    "#original depths [[25, 50, 100, 250, None]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest Parameters to seach over\n",
    "n_estimators_RF = [500, 525, 550, 600, 750, 1000] #list of estimators\n",
    "max_depth_RF = [25, 10, None] #list of depths\n",
    "num_param_RF = len(n_estimators_RF)*len(max_depth_RF) #calculates number of potentials\n",
    "\n",
    "# To identify the best set of parameters later\n",
    "param_RF = []\n",
    "\n",
    "# For storing RF results\n",
    "precision_RF = np.zeros((num_param_RF,n_folds), dtype=float) #an array of float 0s is made with\n",
    "recall_RF = np.zeros((num_param_RF,n_folds), dtype=float)\n",
    "fscore_RF = np.zeros((num_param_RF,n_folds), dtype=float)\n",
    "accuracy_RF = np.zeros((num_param_RF,n_folds), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original n_estimators_GB = [10, 50, 100, 150, 200]\n",
    "#original depth_GB = [10, 20, 30, 50, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Gradient Boost Parameters to seach over\n",
    "n_estimators_GB = [200, 250, 300, 350, 400]\n",
    "depth_GB = [10, None]\n",
    "num_param_GB = len(n_estimators_GB)*len(depth_GB)\n",
    "\n",
    "# To identify the best set of parameters later\n",
    "param_GB = []\n",
    "\n",
    "# For storing RF results\n",
    "precision_GB = np.zeros((num_param_GB,n_folds), dtype=float)\n",
    "recall_GB = np.zeros((num_param_GB,n_folds), dtype=float)\n",
    "fscore_GB = np.zeros((num_param_GB,n_folds), dtype=float)\n",
    "accuracy_GB = np.zeros((num_param_GB,n_folds), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM rbf\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "cost = [0.0001, 0.001, 0.01, 0.1, 1, 10] # Keep SVM cost at a max of 10, any bigger and you are most likely overfitting\n",
    "num_param_SVM = len(cost)\n",
    "\n",
    "# To identify the best set of parameters later\n",
    "param_SVM = []\n",
    "\n",
    "precision_SVM = np.zeros((num_param_SVM,n_folds), dtype=float)\n",
    "recall_SVM = np.zeros((num_param_SVM,n_folds), dtype=float)\n",
    "fscore_SVM = np.zeros((num_param_SVM,n_folds), dtype=float)\n",
    "accuracy_SVM = np.zeros((num_param_SVM,n_folds), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Linear\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "cost = [0.0001, 0.001, 0.01, 0.1, 1, 10] # Keep SVM cost at a max of 10, any bigger and you are most likely overfitting\n",
    "num_param_SVML = len(cost)\n",
    "\n",
    "# To identify the best set of parameters later\n",
    "param_SVML = []\n",
    "\n",
    "precision_SVML = np.zeros((num_param_SVM,n_folds), dtype=float)\n",
    "recall_SVML = np.zeros((num_param_SVM,n_folds), dtype=float)\n",
    "fscore_SVML = np.zeros((num_param_SVM,n_folds), dtype=float)\n",
    "accuracy_SVML = np.zeros((num_param_SVM,n_folds), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................. Random Forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:09<00:00, 11.56s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................. Gradient Boost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [37:38<00:00, 451.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................. Random Forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:45<00:00,  7.52s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................. Gradient Boost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [22:38<00:00, 271.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................. Random Forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:40<00:00,  6.67s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................. Gradient Boost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [22:00<00:00, 264.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................. Random Forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:46<00:00,  7.80s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................. Gradient Boost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [23:17<00:00, 279.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................. Random Forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:45<00:00,  7.53s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................. Gradient Boost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [31:58<00:00, 383.74s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# For monitoring training\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm #progress bars\n",
    "\n",
    "fld_cnt = 0\n",
    "for train_index, test_index in kf_Strat.split(df[cols], df['stig_label']):\n",
    "    \n",
    "    print(\"Processing fold \" + str((fld_cnt+1)))\n",
    "    \n",
    "    cols = df[df.columns.difference([\"stig_label\"])].columns\n",
    "    \n",
    "    X_train_CFV = df.loc[df.index[train_index],cols]\n",
    "    y_train_CFV =df.loc[df.index[train_index],'stig_label']\n",
    "    X_test_CFV = df.loc[df.index[test_index],cols]\n",
    "    y_test_CFV =df.loc[df.index[test_index],'stig_label']\n",
    "    \n",
    "    cols = df[df.columns.difference([\"stig_label\", \"body_text\"])].columns\n",
    "\n",
    "    # instantiate the vectorizer\n",
    "    tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "    # learn training data vocabulary, then use it to create a document-term matrix\n",
    "    tfidf_vect_fit = tfidf_vect.fit(X_train_CFV['body_text'])\n",
    "    \n",
    "    tfidf_train_CFV = tfidf_vect_fit.transform(X_train_CFV['body_text'])\n",
    "\n",
    "    # transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "    tfidf_test_CFV = tfidf_vect_fit.transform(X_test_CFV['body_text'])\n",
    "\n",
    "    X_train_CFV_vect = pd.concat([X_train_CFV[cols].reset_index(drop=True), \n",
    "               pd.DataFrame(tfidf_train_CFV.toarray(), columns=tfidf_vect.get_feature_names())], axis=1)\n",
    "\n",
    "    X_test_CFV_vect = pd.concat([X_test_CFV[cols].reset_index(drop=True), \n",
    "               pd.DataFrame(tfidf_test_CFV.toarray(), columns=tfidf_vect.get_feature_names())], axis=1) \n",
    "    \n",
    "    # Scale the data to reduce influence of features with large values and to speed up training\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    X_train_CFV_vect = min_max_scaler.fit_transform(X_train_CFV_vect)\n",
    "    X_test_CFV_vect = min_max_scaler.transform(X_test_CFV_vect)   \n",
    "\n",
    "\n",
    "    # Perform Random Forest Grid Search and store results \n",
    "    print(\"................. Random Forest\")\n",
    "    grid_cnt = 0\n",
    "    for est in tqdm(n_estimators_RF):\n",
    "        for dpth in max_depth_RF:\n",
    "            \n",
    "            if fld_cnt == 0:\n",
    "                param_RF.append([est, dpth])\n",
    "\n",
    "            rf_CFV = RandomForestClassifier(n_estimators=est, max_depth=dpth, n_jobs=-1, random_state=0) \n",
    "            rf_model_CFV = rf_CFV.fit(X_train_CFV_vect, y_train_CFV)\n",
    "\n",
    "            y_pred_CFV_RF = rf_model_CFV.predict(X_test_CFV_vect)\n",
    "\n",
    "            precision_RF[grid_cnt, fld_cnt], recall_RF[grid_cnt, fld_cnt], fscore_RF[grid_cnt, fld_cnt], train_support = score(y_test_CFV, y_pred_CFV_RF, pos_label=1, average=\"binary\")\n",
    "            accuracy_RF[grid_cnt, fld_cnt] = (y_pred_CFV_RF==y_test_CFV).sum()/len(y_pred_CFV_RF)\n",
    "\n",
    "            grid_cnt += 1\n",
    "                \n",
    "    # Perform Gradient Boost Grid Search and store results     \n",
    "    print(\"................. Gradient Boost\")\n",
    "    grid_cnt = 0\n",
    "    for n_est in tqdm(n_estimators_GB):\n",
    "        for depth in depth_GB:\n",
    "            \n",
    "            if fld_cnt == 0:\n",
    "                param_GB.append([n_est, depth])\n",
    "            \n",
    "            gb_CFV = GradientBoostingClassifier(n_estimators=n_est, max_depth=depth, random_state=0)\n",
    "            gb_model_CFV = gb_CFV.fit(X_train_CFV_vect, y_train_CFV)\n",
    "\n",
    "            y_pred_CFV_GB = gb_model_CFV.predict(X_test_CFV_vect)\n",
    "\n",
    "            precision_GB[grid_cnt, fld_cnt], recall_GB[grid_cnt, fld_cnt], fscore_GB[grid_cnt, fld_cnt], train_support = score(y_test_CFV, y_pred_CFV_GB, pos_label=1, average=\"binary\")\n",
    "            accuracy_GB[grid_cnt, fld_cnt] = (y_pred_CFV_GB==y_test_CFV).sum()/len(y_pred_CFV_GB)\n",
    "\n",
    "            grid_cnt += 1   \n",
    "            \n",
    "            \n",
    "    #print(\"................. RBF SVM\")     \n",
    "    #grid_cnt = 0\n",
    "    #for cst in tqdm(cost):\n",
    "        \n",
    "    #    if fld_cnt == 0:\n",
    "    #        param_SVM.append(cst)\n",
    "    \n",
    "   #     svmClas_CFV = SVC(C = cst, probability=True, random_state=0)\n",
    "        \n",
    "    #    svmClas_model_CFV = svmClas_CFV.fit(X_train_CFV_vect, y_train_CFV)\n",
    "        \n",
    "     #   y_pred_CFV_SVM = svmClas_model_CFV.predict(X_test_CFV_vect)\n",
    "    \n",
    "      #  precision_SVM[grid_cnt, fld_cnt], recall_SVM[grid_cnt, fld_cnt], fscore_SVM[grid_cnt, fld_cnt], train_support = score(y_test_CFV, y_pred_CFV_SVM, pos_label=1, average=\"binary\")\n",
    "       # accuracy_SVM[grid_cnt, fld_cnt] = (y_pred_CFV_SVM==y_test_CFV).sum()/len(y_pred_CFV_SVM)            \n",
    "            \n",
    "       # grid_cnt += 1  \n",
    "    \n",
    "    #print(\"................. Linear SVM\")     \n",
    "    #grid_cnt = 0\n",
    "    #for cst in tqdm(cost):\n",
    "        \n",
    "     #   if fld_cnt == 0:\n",
    "      #      param_SVML.append(cst)\n",
    "    \n",
    "       # svmClas_CFVL = SVC(kernel='linear', C = cst, probability=True, random_state=0)\n",
    "        \n",
    "        #svmClas_model_CFVL = svmClas_CFVL.fit(X_train_CFV_vect, y_train_CFV)\n",
    "        \n",
    "        #y_pred_CFV_SVML = svmClas_model_CFV.predict(X_test_CFV_vect)\n",
    "    \n",
    "        #precision_SVML[grid_cnt, fld_cnt], recall_SVML[grid_cnt, fld_cnt], fscore_SVML[grid_cnt, fld_cnt], train_support = score(y_test_CFV, y_pred_CFV_SVML, pos_label=1, average=\"binary\")\n",
    "        #accuracy_SVML[grid_cnt, fld_cnt] = (y_pred_CFV_SVML==y_test_CFV).sum()/len(y_pred_CFV_SVML)            \n",
    "            \n",
    "        #grid_cnt += 1  \n",
    "\n",
    "    #clear_output(wait=True)                      \n",
    "    fld_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF best n_estimators is: 500 best max_depth is: 25\n",
      "RF CFV Training Results: Mean Prec 0.979, Mean Recall 0.956, Mean Fscore 0.967, Mean Accuracy 0.965\n"
     ]
    }
   ],
   "source": [
    "# choosing best RF paramters \n",
    "prec_final_RF = np.mean(precision_RF, axis = 1)\n",
    "recall_final_RF = np.mean(recall_RF, axis = 1)        \n",
    "fscore_final_RF = np.mean(fscore_RF, axis = 1)  \n",
    "accuracy_final_RF = np.mean(accuracy_RF, axis = 1)  \n",
    "\n",
    "# Choosing final system set-up based on best F-score\n",
    "max_ind_RF = np.argmax(accuracy_final_RF)\n",
    "final_param_RF = param_RF[max_ind_RF]  \n",
    "\n",
    "print(\"RF best n_estimators is: \" + str(final_param_RF[0]) + \" best max_depth is: \" + str(final_param_RF[1]))\n",
    "\n",
    "prec_best_RF = prec_final_RF[max_ind_RF]\n",
    "recall_best_RF = recall_final_RF[max_ind_RF]\n",
    "fscore_best_RF = fscore_final_RF[max_ind_RF]\n",
    "accuracy_best_RF = accuracy_final_RF[max_ind_RF]\n",
    "\n",
    "print( \"RF CFV Training Results: Mean Prec \" + str(round(prec_best_RF,3)) + \n",
    "      \", Mean Recall \" + str(round(recall_best_RF,3)) + \n",
    "      \", Mean Fscore \" + str(round(fscore_best_RF,3)) + \n",
    "      \", Mean Accuracy \" + str(round(accuracy_best_RF,3)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB best n_estimators is: 200 best depth is: 10\n",
      "RF CFV Training Results: Mean Prec 0.966, Mean Recall 0.938, Mean Fscore 0.952, Mean Accuracy 0.948\n"
     ]
    }
   ],
   "source": [
    "# choosing best GB paramters \n",
    "prec_final_GB = np.mean(precision_GB, axis = 1)\n",
    "recall_final_GB = np.mean(recall_GB, axis = 1)        \n",
    "fscore_final_GB = np.mean(fscore_GB, axis = 1)  \n",
    "accuracy_final_GB = np.mean(accuracy_GB, axis = 1)  \n",
    "\n",
    "# Choosing final system set-up based on best F-score\n",
    "max_ind_GB = np.argmax(accuracy_final_GB)\n",
    "final_param_GB = param_GB[max_ind_GB]  \n",
    "\n",
    "print(\"GB best n_estimators is: \" + str(final_param_GB[0]) + \" best depth is: \" + str(final_param_GB[1]))\n",
    "\n",
    "prec_best_GB = prec_final_GB[max_ind_GB]\n",
    "recall_best_GB = recall_final_GB[max_ind_GB]\n",
    "fscore_best_GB = fscore_final_GB[max_ind_GB]\n",
    "accuracy_best_GB = accuracy_final_GB[max_ind_GB]\n",
    "\n",
    "print( \"RF CFV Training Results: Mean Prec \" + str(round(prec_best_GB,3)) + \n",
    "      \", Mean Recall \" + str(round(recall_best_GB,3)) + \n",
    "      \", Mean Fscore \" + str(round(fscore_best_GB,3)) + \n",
    "      \", Mean Accuracy \" + str(round(accuracy_best_GB,3)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM best cost setting is: 10\n",
      "SVM CFV Training Results: Mean Prec 0.988, Mean Recall 0.931, Mean Fscore 0.958, Mean Accuracy 0.955\n"
     ]
    }
   ],
   "source": [
    "# choosing best SVM paramters \n",
    "prec_final_SVM = np.mean(precision_SVM, axis = 1)\n",
    "recall_final_SVM = np.mean(recall_SVM, axis = 1)        \n",
    "fscore_final_SVM = np.mean(fscore_SVM, axis = 1)  \n",
    "accuracy_final_SVM = np.mean(accuracy_SVM, axis = 1) \n",
    "\n",
    "# Choosing final system set-up based on best F-score\n",
    "max_ind_SVM = np.argmax(accuracy_final_SVM)\n",
    "final_param_SVM = param_SVM[max_ind_SVM]  \n",
    "\n",
    "print(\"SVM best cost setting is: \" + str(final_param_SVM))\n",
    "\n",
    "prec_best_SVM = prec_final_SVM[max_ind_SVM]\n",
    "recall_best_SVM = recall_final_SVM[max_ind_SVM]\n",
    "fscore_best_SVM = fscore_final_SVM[max_ind_SVM]\n",
    "accuracy_best_SVM = accuracy_final_SVM[max_ind_SVM]\n",
    "\n",
    "print( \"SVM CFV Training Results: Mean Prec \" + str(round(prec_best_SVM,3)) + \n",
    "      \", Mean Recall \" + str(round(recall_best_SVM,3)) + \n",
    "      \", Mean Fscore \" + str(round(fscore_best_SVM,3)) + \n",
    "      \", Mean Accuracy \" + str(round(accuracy_best_SVM,3)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM linear best cost setting is: 0.0001\n",
      "SVM linear CFV Training Results: Mean Prec 0.988, Mean Recall 0.931, Mean Fscore 0.958, Mean Accuracy 0.955\n"
     ]
    }
   ],
   "source": [
    "# choosing best SVML paramters \n",
    "prec_final_SVML = np.mean(precision_SVML, axis = 1)\n",
    "recall_final_SVML = np.mean(recall_SVML, axis = 1)        \n",
    "fscore_final_SVML = np.mean(fscore_SVML, axis = 1)  \n",
    "accuracy_final_SVML = np.mean(accuracy_SVML, axis = 1) \n",
    "\n",
    "# Choosing final system set-up based on best F-score\n",
    "max_ind_SVML = np.argmax(accuracy_final_SVML)\n",
    "final_param_SVML = param_SVML[max_ind_SVML]  \n",
    "\n",
    "print(\"SVM linear best cost setting is: \" + str(final_param_SVML))\n",
    "\n",
    "prec_best_SVML = prec_final_SVML[max_ind_SVML]\n",
    "recall_best_SVML = recall_final_SVML[max_ind_SVML]\n",
    "fscore_best_SVML = fscore_final_SVML[max_ind_SVML]\n",
    "accuracy_best_SVML = accuracy_final_SVML[max_ind_SVML]\n",
    "\n",
    "print( \"SVM linear CFV Training Results: Mean Prec \" + str(round(prec_best_SVML,3)) + \n",
    "      \", Mean Recall \" + str(round(recall_best_SVML,3)) + \n",
    "      \", Mean Fscore \" + str(round(fscore_best_SVML,3)) + \n",
    "      \", Mean Accuracy \" + str(round(accuracy_best_SVML,3)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of Nick's Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    " def conFusion_metricsOutput(var_y_test, var_y_pred, modname):\n",
    "    \n",
    "    res = []   \n",
    "    confusion = metrics.confusion_matrix(var_y_test, var_y_pred)\n",
    "    TP = confusion[1, 1] #True Positives (TP): we correctly predicted that tweets do have stigma\n",
    "    TN = confusion[0, 0] #True Negatives (TN): we correctly predicted that tweets don't have stigma\n",
    "    FP = confusion[0, 1] #False Positives (FP): we incorrectly predicted that tweets do have stigma (a \"Type I error\")\n",
    "    FN = confusion[1, 0] #False Negatives (FN): we incorrectly predicted that tweets don't have stigma (a \"Type II error\")\n",
    "\n",
    "    ## Classification Accuracy: Overall, how often is the classifier correct?\n",
    "    accuracy = ((TP + TN) / float(TP + TN + FP + FN))\n",
    "    #print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    #Classification Error: Overall, how often is the classifier incorrect?\n",
    "    #Also known as \"Misclassification Rate\"\n",
    "    misclassication_rate = ((FP + FN) / float(TP + TN + FP + FN))\n",
    "    #print(1 - metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    #Recall/Sensitivity: When the actual value is positive, how often is the prediction correct?\n",
    "    #How \"sensitive\" is the classifier to detecting positive instances?\n",
    "    #Also known as \"True Positive Rate\"\n",
    "    recall_tpr = (TP / float(TP + FN))\n",
    "    #print(metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "    #Specificity: When the actual value is negative, how often is the prediction correct?\n",
    "    #How \"specific\" (or \"selective\") is the classifier in predicting positive instances?\n",
    "    specificity = (TN / float(TN + FP))\n",
    "\n",
    "    #False Positive Rate: When the actual value is negative, how often is the prediction incorrect?\n",
    "    fpr = (FP / float(TN + FP))\n",
    "\n",
    "    #Precision: When a positive value is predicted, how often is the prediction correct?\n",
    "    #How \"precise\" is the classifier when predicting positive instances?\n",
    "    precision = (TP / float(TP + FP))\n",
    "    #print(metrics.precision_score(y_test, y_pred))\n",
    "    \n",
    "    fnr = (FN / float(TP + FN))\n",
    "    \n",
    "    false_negatives = FN\n",
    "    \n",
    "    false_positives = FP\n",
    "    \n",
    "    res.append([accuracy, misclassication_rate, recall_tpr, specificity, fpr, precision, fnr, false_negatives, false_positives])\n",
    "    \n",
    "    data=pd.DataFrame(res ,columns=['accuracy','misclassication_rate', 'recall_tpr' , 'specificity' , 'fpr', 'precision', 'fnr', 'false_negatives', 'false_positives'], index=[modname]).T\n",
    "    print(TP, TN, FP, FN)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1, random_state=0)\n",
    "k_fold = KFold(n_splits=5)\n",
    "\n",
    "y_pred_rf_cv = cross_val_predict(rf, X_features_cv, df['stig_label'], cv=k_fold, n_jobs = -1)\n",
    "\n",
    "auc = roc_auc_score(df['stig_label'], y_pred_rf_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = conFusion_metricsOutput(df['stig_label'], y_pred_rf_cv, modname = 'rf_cv')\n",
    "res_df\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#individual k-folds\n",
    "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1, random_state=0)\n",
    "k_fold = KFold(n_splits=5)\n",
    "%time accuracy =        cross_val_score(rf, X_features_cv, df['stig_label'], cv = k_fold, n_jobs = -1, scoring='accuracy')\n",
    "%time precision_score =  cross_val_score(rf,  X_features_cv, df['stig_label'], cv = k_fold, n_jobs = -1, scoring='precision')\n",
    "%time recall_score =    cross_val_score(rf,  X_features_cv, df['stig_label'], cv = k_fold, n_jobs = -1, scoring='recall')\n",
    "\n",
    "\n",
    "print(accuracy, precision_score, recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RF(n_est, depth):\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs=-1, random_state=0)\n",
    "    rf_model = rf.fit(X_train_vect, y_train)\n",
    "    y_pred = rf_model.predict(X_test_vect)\n",
    "    precision, recall, fscore, train_support = score(y_test, y_pred, pos_label=1, average=\"binary\")\n",
    "    print('Est: {} / Depth: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    n_est, depth, precision.mean(), recall.mean(),\n",
    "    (y_pred==y_test).sum() / len(y_pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extend if extremes show best\n",
    "%precision %.2f\n",
    "np.set_printoptions(precision=2)\n",
    "#200 estimators with 20 depth gives best results first time so redo up to 250\n",
    "\n",
    "for n_est in [10, 50, 100, 150, 200, 250]:\n",
    "    for depth in [10, 20, 30, 50, None]:\n",
    "        train_RF(n_est, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best is 200 est, depth 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf hold out test\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=20, n_jobs=-1, random_state=0) \n",
    "\n",
    "rf_model = rf.fit(X_train_vect, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test_vect)\n",
    "auc = roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = res_df.join(conFusion_metricsOutput(y_test, y_pred, 'rf_holdout'))\n",
    "\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=150, max_depth=None, random_state=0)\n",
    "k_fold = KFold(n_splits=5)\n",
    "\n",
    "y_pred_gb_cv = cross_val_predict(gb, X_features_cv, df['stig_label'], cv=k_fold, n_jobs = -1)\n",
    "\n",
    "auc = roc_auc_score(df['stig_label'], y_pred_gb_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = res_df.join(conFusion_metricsOutput(df['stig_label'], y_pred_gb_cv, modname = 'gb_cv'))\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GB(n_est, depth):\n",
    "    gb = GradientBoostingClassifier(n_estimators=n_est, max_depth=depth, random_state=0)\n",
    "    gb_model = gb.fit(X_train_vect, y_train)\n",
    "    y_pred = gb_model.predict(X_test_vect)\n",
    "    precision, recall, fscore, train_support = score(y_test, y_pred, pos_label=1, average=\"binary\")\n",
    "    print('Est: {} / Depth: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    n_est, depth, precision.mean(), recall.mean(),\n",
    "    (y_pred==y_test).sum() / len(y_pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_est in [10, 50, 100, 150, 200]:\n",
    "    for depth in [10, 20, 30, 50, None]:\n",
    "        train_GB(n_est, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best for GB is 10 estimators depth 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=10, max_depth=10, random_state=0)\n",
    "\n",
    "\n",
    "gb_model = gb.fit(X_train_vect, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = gb_model.predict(X_test_vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = res_df.join(conFusion_metricsOutput(y_test, y_pred, 'gb_holdout'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmClas = SVC(probability=True)\n",
    "\n",
    "y_pred_svm_cv = cross_val_predict(svmClas, X_features_cv, df['stig_label'], cv=k_fold, n_jobs = -1)\n",
    "\n",
    "auc = roc_auc_score(df['stig_label'], y_pred_svm_cv)\n",
    "\n",
    "res_df = res_df.join(conFusion_metricsOutput(df['stig_label'], y_pred_svm_cv, modname = 'svm_cv'))\n",
    "res_df\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_cost = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 100000, 1000000]\n",
    "\n",
    "for cst in SVM_cost:\n",
    "\n",
    "    svmClas = SVC(C = cst, random_state=0)\n",
    "\n",
    "    svmClas.fit(X_train_vect, y_train)\n",
    "\n",
    "    y_pred = svmClas.predict(X_test_vect)\n",
    "    precision, recall, fscore, train_support = score(y_test, y_pred, pos_label=1, average=\"binary\")\n",
    "    print('Cost function: {} --- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    cst, precision.mean(), recall.mean(),\n",
    "    (y_pred==y_test).sum() / len(y_pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function of 10000 is best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmClas = SVC(C = 10000, random_state=0)\n",
    "svmClas.fit(X_train_vect, y_train)\n",
    "\n",
    "pred = svmClas.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = res_df.join(conFusion_metricsOutput(y_test, pred, 'svm_holdout'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(y_test, pred)\n",
    "\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmClas = SVC(kernel='linear', probability=True)\n",
    "y_pred_svm_lin_cv = cross_val_predict(svmClas, X_features_cv, df['stig_label'], cv=k_fold, n_jobs = -1)\n",
    "\n",
    "auc = roc_auc_score(df['stig_label'], y_pred_svm_lin_cv)\n",
    "\n",
    "res_df = res_df.join(conFusion_metricsOutput(df['stig_label'], y_pred_svm_lin_cv, modname = 'svm_lin_cv'))\n",
    "res_df\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_cost = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "for cst in SVM_cost:\n",
    "\n",
    "    svmClas = SVC(kernel='linear', C = cst, random_state=0)\n",
    "\n",
    "    svmClas.fit(X_train_vect, y_train)\n",
    "    y_pred = svmClas.predict(X_test_vect)\n",
    "    precision, recall, fscore, train_support = score(y_test, y_pred, pos_label=1, average=\"binary\")\n",
    "    print('Cost function: {} --- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    cst, precision.mean(), recall.mean(),\n",
    "    (y_pred==y_test).sum() / len(y_pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best cost function for SVM linear is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmClas = SVC(kernel='linear', C = 1, random_state=0)\n",
    "svmClas.fit(X_train_vect, y_train)\n",
    "\n",
    "pred = svmClas.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = res_df.join(conFusion_metricsOutput(y_test, pred, 'svm_lin_holdout'))\n",
    "auc = roc_auc_score(y_test, pred)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# according to accuracy and false negatives, Random Forest holdout and SVM linear holdout are best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
