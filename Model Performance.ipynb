{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "##Explore this blog: https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html for visualisation ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads rated tweets into dataframe\n",
    "df = pd.read_excel(\"C:/Users/ertur/Documents/Work/Workwork/ARUK/Submission - JMIR Aging/Revisions/Categorised tweets 1500.xlsx\", converters={'Tweet':str,'Theme':int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'Tweet':'body_text', 'Theme':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1497, 2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing cases where rating is missing\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining sentiment and subjectivity\n",
    "def sentAnal(df):\n",
    "    for index, row in df.iterrows():\n",
    "        temp = TextBlob(row['body_text'])\n",
    "        df.loc[index,'Sentiment'] = temp.sentiment.polarity\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sentAnal(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1414, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing tweets rated as uncertain or unknown\n",
    "themes=[1,2,3,4,5,6]\n",
    "df = df[df.label.isin(themes)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting assigned themes into corresponding rating of stigmatising and non-stigmatising\n",
    "theme_map = {1:0, 2:0, 3:0, 4:1, 5:1, 6:1}\n",
    "df['stig_label'] = df.label.map(theme_map)\n",
    "df = df.drop('label', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#literature defined features are generated\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "df['body_len'] = df['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "df['punct%'] = df['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "# Average Word Length. simply take the sum of the length of all the words and divide it by the total length of the tweet as defined in function above\n",
    "df['avg_word'] = df['body_text'].apply(lambda x: avg_word(x))\n",
    "\n",
    "# Number of Words in tweet\n",
    "df['word_count'] = df['body_text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "# Number of characters. Here, we calculate the number of characters in each tweet. This is done by calculating the length of the tweet.\n",
    "df['char_count'] = df['body_text'].str.len() ## this also includes spaces\n",
    "\n",
    "# number of special characters like hashtags. we make use of the ‘starts with’ function because hashtags (or mentions) always appear at the beginning of a word.\n",
    "df['hastags'] = df['body_text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "\n",
    "# number of numerics in tweet\n",
    "df['numerics'] = df['body_text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#care-partner defined features are generated\n",
    "#senile\n",
    "Search_for_These_values = ['senile', 'SENILE'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['senile'] = df['body_text'].str.contains(pattern)\n",
    "df['senile'] = df['senile'].map({True: 1, False: 0})\n",
    "#demented\n",
    "Search_for_These_values = ['demented', 'DEMENTED'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['demented'] = df['body_text'].str.contains(pattern)\n",
    "df['demented'] = df['demented'].map({True: 1, False: 0})\n",
    "#donald trump\n",
    "Search_for_These_values = ['donald', 'trump', 'DONALD', 'TRUMP', '@realDonaldTrump'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['donaldtrump'] = df['body_text'].str.contains(pattern)\n",
    "df['donaldtrump'] = df['donaldtrump'].map({True: 1, False: 0})\n",
    "#memory\n",
    "Search_for_These_values = ['MEMORY', 'memory'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Memory'] = df['body_text'].str.contains(pattern)\n",
    "df['Memory'] = df['Memory'].map({True: 1, False: 0})\n",
    "#research\n",
    "Search_for_These_values = ['research', 'RESEARCH'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Research'] = df['body_text'].str.contains(pattern)\n",
    "df['Research'] = df['Research'].map({True: 1, False: 0})\n",
    "#crazy\n",
    "Search_for_These_values = ['crazy', 'CRAZY'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Crazy'] = df['body_text'].str.contains(pattern)\n",
    "df['Crazy'] = df['Crazy'].map({True: 1, False: 0})\n",
    "#senility\n",
    "Search_for_These_values = ['senility', 'SENILITY'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Senility'] = df['body_text'].str.contains(pattern)\n",
    "df['Senility'] = df['Senility'].map({True: 1, False: 0})\n",
    "# URL\n",
    "Search_for_These_values = ['https'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Link'] = df['body_text'].str.contains(pattern)\n",
    "df['Link'] = df['Link'].map({True: 1, False: 0})\n",
    "#caregiver\n",
    "Search_for_These_values = ['caregiver', 'CAREGIVER'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Caregiver'] = df['body_text'].str.contains(pattern)\n",
    "df['Caregiver'] = df['Caregiver'].map({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1414, 19)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df[df.columns.difference([\"stig_label\"])].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[cols], df['stig_label'], test_size=0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df[df.columns.difference([\"stig_label\", \"body_text\"])].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of each training and testing datasets are:\n",
      "(1131, 18)\n",
      "(283, 18)\n",
      "(1131,)\n",
      "(283,)\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of each training and testing datasets are:\")\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the vectorizer\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "tfidf_vect_fit = tfidf_vect.fit(X_train['body_text'])\n",
    "tfidf_train = tfidf_vect_fit.transform(X_train['body_text'])\n",
    "\n",
    "\n",
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "tfidf_test = tfidf_vect_fit.transform(X_test['body_text'])\n",
    "\n",
    "X_train_vect = pd.concat([X_train[cols].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_train.toarray(), columns=tfidf_vect.get_feature_names())], axis=1)\n",
    "\n",
    "X_test_vect = pd.concat([X_test[cols].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_test.toarray(), columns=tfidf_vect.get_feature_names())], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Fold Validation Code from Nick**, it's slower but everything is comparable and the test vectors have been vectorised by the training vectors. I've not included all modesl, but you should be able to see the pattern of how to do it if you want to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_folds = 5 # number of folds to do being set\n",
    "kf_Strat = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=0) #sets this as a 'train test split equiv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1\n",
      "Processing fold 2\n",
      "Processing fold 3\n",
      "Processing fold 4\n",
      "Processing fold 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# For monitoring training\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm #progress bars\n",
    "\n",
    "results = {}\n",
    "fld_cnt = 1\n",
    "for train_index, test_index in kf_Strat.split(df[cols], df['stig_label']):\n",
    "    \n",
    "    print(\"Processing fold \" + str((fld_cnt)))\n",
    "    \n",
    "    cols = df[df.columns.difference([\"stig_label\"])].columns\n",
    "    \n",
    "    X_train_CFV = df.loc[df.index[train_index],cols]\n",
    "    y_train_CFV =df.loc[df.index[train_index],'stig_label']\n",
    "    X_test_CFV = df.loc[df.index[test_index],cols]\n",
    "    y_test_CFV =df.loc[df.index[test_index],'stig_label']\n",
    "    \n",
    "    cols = df[df.columns.difference([\"stig_label\", \"body_text\"])].columns\n",
    "\n",
    "    # instantiate the vectorizer\n",
    "    tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "    # learn training data vocabulary, then use it to create a document-term matrix\n",
    "    tfidf_vect_fit = tfidf_vect.fit(X_train_CFV['body_text'])\n",
    "    \n",
    "    tfidf_train_CFV = tfidf_vect_fit.transform(X_train_CFV['body_text'])\n",
    "\n",
    "    # transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "    tfidf_test_CFV = tfidf_vect_fit.transform(X_test_CFV['body_text'])\n",
    "\n",
    "    X_train_CFV_vect = pd.concat([X_train_CFV[cols].reset_index(drop=True), \n",
    "               pd.DataFrame(tfidf_train_CFV.toarray(), columns=tfidf_vect.get_feature_names())], axis=1)\n",
    "\n",
    "    X_test_CFV_vect = pd.concat([X_test_CFV[cols].reset_index(drop=True), \n",
    "               pd.DataFrame(tfidf_test_CFV.toarray(), columns=tfidf_vect.get_feature_names())], axis=1) \n",
    "    \n",
    "    # Scale the data to reduce influence of features with large values and to speed up training\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    X_train_CFV_vect = min_max_scaler.fit_transform(X_train_CFV_vect)\n",
    "    X_test_CFV_vect = min_max_scaler.transform(X_test_CFV_vect)   \n",
    "    \n",
    "    rf_CFV = RandomForestClassifier(n_estimators=500, max_depth=25, n_jobs=-1, random_state=0)\n",
    "    \n",
    "    rf_model_CFV = rf_CFV.fit(X_train_CFV_vect, y_train_CFV)\n",
    "    y_pred_CFV_RF = rf_model_CFV.predict(X_test_CFV_vect)\n",
    "    confusion = metrics.confusion_matrix(y_test_CFV, y_pred_CFV_RF)\n",
    "    TP = confusion[1, 1] #True Positives (TP): we correctly predicted that tweets do have stigma\n",
    "    TN = confusion[0, 0] #True Negatives (TN): we correctly predicted that tweets don't have stigma\n",
    "    FP = confusion[0, 1] #False Positives (FP): we incorrectly predicted that tweets do have stigma (a \"Type I error\")\n",
    "    FN = confusion[1, 0]\n",
    "    auc = roc_auc_score(y_test_CFV, y_pred_CFV_RF)\n",
    "    results[fld_cnt] = [TP,TN,FP,FN, auc]\n",
    "    fld_cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [150, 124, 4, 5, 0.9682459677419355],\n",
       " 2: [145, 125, 3, 10, 0.956023185483871],\n",
       " 3: [151, 124, 3, 5, 0.9721633353523118],\n",
       " 4: [145, 125, 2, 11, 0.9568695739955584],\n",
       " 5: [152, 123, 4, 3, 0.9745745491490984]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF CV results:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9646467684134026,\n",
       " 0.0353532315865975,\n",
       " 0.9562613730355667,\n",
       " 0.9748892716535433,\n",
       " 0.02511072834645669,\n",
       " 0.9790057432914574,\n",
       " 0.04373862696443341,\n",
       " 6.8,\n",
       " 3.2,\n",
       " 0.9655753223445551]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in results:\n",
    "    temp = results[i]\n",
    "    TP = temp[0]\n",
    "    TN = temp[1]\n",
    "    FP = temp[2]\n",
    "    FN = temp[3]\n",
    "    auc_t= temp[4]\n",
    "    if i == 1:\n",
    "        accuracy = ((TP + TN) / float(TP + TN + FP + FN))\n",
    "        misclassication_rate = ((FP + FN) / float(TP + TN + FP + FN))\n",
    "        recall_tpr = (TP / float(TP + FN))\n",
    "        specificity = (TN / float(TN + FP))\n",
    "        fpr = (FP / float(TN + FP))\n",
    "        precision = (TP / float(TP + FP))\n",
    "        fnr = (FN / float(TP + FN))\n",
    "        false_negatives = FN\n",
    "        false_positives = FP\n",
    "        auc = auc_t\n",
    "    else:\n",
    "        accuracy += ((TP + TN) / float(TP + TN + FP + FN))\n",
    "        misclassication_rate += ((FP + FN) / float(TP + TN + FP + FN))\n",
    "        recall_tpr += (TP / float(TP + FN))\n",
    "        specificity += (TN / float(TN + FP))\n",
    "        fpr += (FP / float(TN + FP))\n",
    "        precision += (TP / float(TP + FP))\n",
    "        fnr += (FN / float(TP + FN))\n",
    "        false_negatives += FN\n",
    "        false_positives += FP\n",
    "        auc += auc_t\n",
    "res = [accuracy/5, misclassication_rate/5, recall_tpr/5, specificity/5, fpr/5,  precision/5, fnr/5, false_negatives/5, false_positives/5, auc/5 ]\n",
    "print('RF CV results:')\n",
    "res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1\n",
      "Processing fold 2\n",
      "Processing fold 3\n",
      "Processing fold 4\n",
      "Processing fold 5\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "fld_cnt = 1\n",
    "for train_index, test_index in kf_Strat.split(df[cols], df['stig_label']):\n",
    "    \n",
    "    print(\"Processing fold \" + str((fld_cnt)))\n",
    "    \n",
    "    cols = df[df.columns.difference([\"stig_label\"])].columns\n",
    "    \n",
    "    X_train_CFV = df.loc[df.index[train_index],cols]\n",
    "    y_train_CFV =df.loc[df.index[train_index],'stig_label']\n",
    "    X_test_CFV = df.loc[df.index[test_index],cols]\n",
    "    y_test_CFV =df.loc[df.index[test_index],'stig_label']\n",
    "    \n",
    "    cols = df[df.columns.difference([\"stig_label\", \"body_text\"])].columns\n",
    "\n",
    "    # instantiate the vectorizer\n",
    "    tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "    # learn training data vocabulary, then use it to create a document-term matrix\n",
    "    tfidf_vect_fit = tfidf_vect.fit(X_train_CFV['body_text'])\n",
    "    \n",
    "    tfidf_train_CFV = tfidf_vect_fit.transform(X_train_CFV['body_text'])\n",
    "\n",
    "    # transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "    tfidf_test_CFV = tfidf_vect_fit.transform(X_test_CFV['body_text'])\n",
    "\n",
    "    X_train_CFV_vect = pd.concat([X_train_CFV[cols].reset_index(drop=True), \n",
    "               pd.DataFrame(tfidf_train_CFV.toarray(), columns=tfidf_vect.get_feature_names())], axis=1)\n",
    "\n",
    "    X_test_CFV_vect = pd.concat([X_test_CFV[cols].reset_index(drop=True), \n",
    "               pd.DataFrame(tfidf_test_CFV.toarray(), columns=tfidf_vect.get_feature_names())], axis=1) \n",
    "    \n",
    "    # Scale the data to reduce influence of features with large values and to speed up training\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    X_train_CFV_vect = min_max_scaler.fit_transform(X_train_CFV_vect)\n",
    "    X_test_CFV_vect = min_max_scaler.transform(X_test_CFV_vect)   \n",
    "    \n",
    "    GB_CFV = GradientBoostingClassifier(n_estimators=200, max_depth=10, random_state=0)\n",
    "    \n",
    "    GB_model_CFV = GB_CFV.fit(X_train_CFV_vect, y_train_CFV)\n",
    "    y_pred_CFV_GB = GB_model_CFV.predict(X_test_CFV_vect)\n",
    "    confusion = metrics.confusion_matrix(y_test_CFV, y_pred_CFV_GB)\n",
    "    TP = confusion[1, 1] #True Positives (TP): we correctly predicted that tweets do have stigma\n",
    "    TN = confusion[0, 0] #True Negatives (TN): we correctly predicted that tweets don't have stigma\n",
    "    FP = confusion[0, 1] #False Positives (FP): we incorrectly predicted that tweets do have stigma (a \"Type I error\")\n",
    "    FN = confusion[1, 0]\n",
    "    auc = roc_auc_score(y_test_CFV, y_pred_CFV_GB)\n",
    "    results[fld_cnt] = [TP,TN,FP,FN, auc]\n",
    "    fld_cnt += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB CV results:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9476655890534547,\n",
       " 0.052334410946545375,\n",
       " 0.9382464846980977,\n",
       " 0.9591535433070867,\n",
       " 0.04084645669291338,\n",
       " 0.9656388604235797,\n",
       " 0.061753515301902394,\n",
       " 9.6,\n",
       " 5.2,\n",
       " 0.9487000140025922]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in results:\n",
    "    temp = results[i]\n",
    "    TP = temp[0]\n",
    "    TN = temp[1]\n",
    "    FP = temp[2]\n",
    "    FN = temp[3]\n",
    "    auc_t= temp[4]\n",
    "    if i == 1:\n",
    "        accuracy = ((TP + TN) / float(TP + TN + FP + FN))\n",
    "        misclassication_rate = ((FP + FN) / float(TP + TN + FP + FN))\n",
    "        recall_tpr = (TP / float(TP + FN))\n",
    "        specificity = (TN / float(TN + FP))\n",
    "        fpr = (FP / float(TN + FP))\n",
    "        precision = (TP / float(TP + FP))\n",
    "        fnr = (FN / float(TP + FN))\n",
    "        false_negatives = FN\n",
    "        false_positives = FP\n",
    "        auc = auc_t\n",
    "    else:\n",
    "        accuracy += ((TP + TN) / float(TP + TN + FP + FN))\n",
    "        misclassication_rate += ((FP + FN) / float(TP + TN + FP + FN))\n",
    "        recall_tpr += (TP / float(TP + FN))\n",
    "        specificity += (TN / float(TN + FP))\n",
    "        fpr += (FP / float(TN + FP))\n",
    "        precision += (TP / float(TP + FP))\n",
    "        fnr += (FN / float(TP + FN))\n",
    "        false_negatives += FN\n",
    "        false_positives += FP\n",
    "        auc += auc_t\n",
    "res = [accuracy/5, misclassication_rate/5, recall_tpr/5, specificity/5, fpr/5,  precision/5, fnr/5, false_negatives/5, false_positives/5, auc/5 ]\n",
    "print('GB CV results:')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1\n",
      "Processing fold 2\n",
      "Processing fold 3\n",
      "Processing fold 4\n",
      "Processing fold 5\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "fld_cnt = 1\n",
    "for train_index, test_index in kf_Strat.split(df[cols], df['stig_label']):\n",
    "    \n",
    "    print(\"Processing fold \" + str((fld_cnt)))\n",
    "    \n",
    "    cols = df[df.columns.difference([\"stig_label\"])].columns\n",
    "    \n",
    "    X_train_CFV = df.loc[df.index[train_index],cols]\n",
    "    y_train_CFV =df.loc[df.index[train_index],'stig_label']\n",
    "    X_test_CFV = df.loc[df.index[test_index],cols]\n",
    "    y_test_CFV =df.loc[df.index[test_index],'stig_label']\n",
    "    \n",
    "    cols = df[df.columns.difference([\"stig_label\", \"body_text\"])].columns\n",
    "\n",
    "    # instantiate the vectorizer\n",
    "    tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "    # learn training data vocabulary, then use it to create a document-term matrix\n",
    "    tfidf_vect_fit = tfidf_vect.fit(X_train_CFV['body_text'])\n",
    "    \n",
    "    tfidf_train_CFV = tfidf_vect_fit.transform(X_train_CFV['body_text'])\n",
    "\n",
    "    # transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "    tfidf_test_CFV = tfidf_vect_fit.transform(X_test_CFV['body_text'])\n",
    "\n",
    "    X_train_CFV_vect = pd.concat([X_train_CFV[cols].reset_index(drop=True), \n",
    "               pd.DataFrame(tfidf_train_CFV.toarray(), columns=tfidf_vect.get_feature_names())], axis=1)\n",
    "\n",
    "    X_test_CFV_vect = pd.concat([X_test_CFV[cols].reset_index(drop=True), \n",
    "               pd.DataFrame(tfidf_test_CFV.toarray(), columns=tfidf_vect.get_feature_names())], axis=1) \n",
    "    \n",
    "    # Scale the data to reduce influence of features with large values and to speed up training\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    X_train_CFV_vect = min_max_scaler.fit_transform(X_train_CFV_vect)\n",
    "    X_test_CFV_vect = min_max_scaler.transform(X_test_CFV_vect)   \n",
    "    \n",
    "    svmClas_CFV = SVC(C = 10, probability=True, random_state=0)\n",
    "    \n",
    "    svmClas_CFV = svmClas_CFV.fit(X_train_CFV_vect, y_train_CFV)\n",
    "    y_pred_CFV_svmClas = svmClas_CFV.predict(X_test_CFV_vect)\n",
    "    confusion = metrics.confusion_matrix(y_test_CFV, y_pred_CFV_svmClas)\n",
    "    TP = confusion[1, 1] #True Positives (TP): we correctly predicted that tweets do have stigma\n",
    "    TN = confusion[0, 0] #True Negatives (TN): we correctly predicted that tweets don't have stigma\n",
    "    FP = confusion[0, 1] #False Positives (FP): we incorrectly predicted that tweets do have stigma (a \"Type I error\")\n",
    "    FN = confusion[1, 0]\n",
    "    auc = roc_auc_score(y_test_CFV, y_pred_CFV_svmClas)\n",
    "    results[fld_cnt] = [TP,TN,FP,FN, auc]\n",
    "    fld_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM RBF CV results:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9554494649525098,\n",
       " 0.04455053504749016,\n",
       " 0.9305045492142267,\n",
       " 0.9858513779527559,\n",
       " 0.014148622047244094,\n",
       " 0.9878592847667569,\n",
       " 0.06949545078577336,\n",
       " 10.8,\n",
       " 1.8,\n",
       " 0.9581779635834913]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in results:\n",
    "    temp = results[i]\n",
    "    TP = temp[0]\n",
    "    TN = temp[1]\n",
    "    FP = temp[2]\n",
    "    FN = temp[3]\n",
    "    auc_t= temp[4]\n",
    "    if i == 1:\n",
    "        accuracy = ((TP + TN) / float(TP + TN + FP + FN))\n",
    "        misclassication_rate = ((FP + FN) / float(TP + TN + FP + FN))\n",
    "        recall_tpr = (TP / float(TP + FN))\n",
    "        specificity = (TN / float(TN + FP))\n",
    "        fpr = (FP / float(TN + FP))\n",
    "        precision = (TP / float(TP + FP))\n",
    "        fnr = (FN / float(TP + FN))\n",
    "        false_negatives = FN\n",
    "        false_positives = FP\n",
    "        auc = auc_t\n",
    "    else:\n",
    "        accuracy += ((TP + TN) / float(TP + TN + FP + FN))\n",
    "        misclassication_rate += ((FP + FN) / float(TP + TN + FP + FN))\n",
    "        recall_tpr += (TP / float(TP + FN))\n",
    "        specificity += (TN / float(TN + FP))\n",
    "        fpr += (FP / float(TN + FP))\n",
    "        precision += (TP / float(TP + FP))\n",
    "        fnr += (FN / float(TP + FN))\n",
    "        false_negatives += FN\n",
    "        false_positives += FP\n",
    "        auc += auc_t\n",
    "res = [accuracy/5, misclassication_rate/5, recall_tpr/5, specificity/5, fpr/5,  precision/5, fnr/5, false_negatives/5, false_positives/5, auc/5 ]\n",
    "print('SVM RBF CV results:')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1\n",
      "Processing fold 2\n",
      "Processing fold 3\n",
      "Processing fold 4\n",
      "Processing fold 5\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "fld_cnt = 1\n",
    "for train_index, test_index in kf_Strat.split(df[cols], df['stig_label']):\n",
    "    \n",
    "    print(\"Processing fold \" + str((fld_cnt)))\n",
    "    \n",
    "    cols = df[df.columns.difference([\"stig_label\"])].columns\n",
    "    \n",
    "    X_train_CFV = df.loc[df.index[train_index],cols]\n",
    "    y_train_CFV =df.loc[df.index[train_index],'stig_label']\n",
    "    X_test_CFV = df.loc[df.index[test_index],cols]\n",
    "    y_test_CFV =df.loc[df.index[test_index],'stig_label']\n",
    "    \n",
    "    cols = df[df.columns.difference([\"stig_label\", \"body_text\"])].columns\n",
    "\n",
    "    # instantiate the vectorizer\n",
    "    tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "    # learn training data vocabulary, then use it to create a document-term matrix\n",
    "    tfidf_vect_fit = tfidf_vect.fit(X_train_CFV['body_text'])\n",
    "    \n",
    "    tfidf_train_CFV = tfidf_vect_fit.transform(X_train_CFV['body_text'])\n",
    "\n",
    "    # transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "    tfidf_test_CFV = tfidf_vect_fit.transform(X_test_CFV['body_text'])\n",
    "\n",
    "    X_train_CFV_vect = pd.concat([X_train_CFV[cols].reset_index(drop=True), \n",
    "               pd.DataFrame(tfidf_train_CFV.toarray(), columns=tfidf_vect.get_feature_names())], axis=1)\n",
    "\n",
    "    X_test_CFV_vect = pd.concat([X_test_CFV[cols].reset_index(drop=True), \n",
    "               pd.DataFrame(tfidf_test_CFV.toarray(), columns=tfidf_vect.get_feature_names())], axis=1) \n",
    "    \n",
    "    # Scale the data to reduce influence of features with large values and to speed up training\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    X_train_CFV_vect = min_max_scaler.fit_transform(X_train_CFV_vect)\n",
    "    X_test_CFV_vect = min_max_scaler.transform(X_test_CFV_vect)   \n",
    "    \n",
    "    svmClasL_CFV = SVC(kernel='linear',C = 0.1, probability=True, random_state=0)\n",
    "    \n",
    "    svmClasL_model_CFV = svmClasL_CFV.fit(X_train_CFV_vect, y_train_CFV)\n",
    "    y_pred_CFV_svmClasL = svmClasL_model_CFV.predict(X_test_CFV_vect)\n",
    "    confusion = metrics.confusion_matrix(y_test_CFV, y_pred_CFV_svmClasL)\n",
    "    TP = confusion[1, 1] #True Positives (TP): we correctly predicted that tweets do have stigma\n",
    "    TN = confusion[0, 0] #True Negatives (TN): we correctly predicted that tweets don't have stigma\n",
    "    FP = confusion[0, 1] #False Positives (FP): we incorrectly predicted that tweets do have stigma (a \"Type I error\")\n",
    "    FN = confusion[1, 0]\n",
    "    auc = roc_auc_score(y_test_CFV, y_pred_CFV_svmClasL)\n",
    "    results[fld_cnt] = [TP,TN,FP,FN, auc]\n",
    "    fld_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Linear CV results:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.960396461418941,\n",
       " 0.03960353858105907,\n",
       " 0.9369396195202647,\n",
       " 0.9890009842519685,\n",
       " 0.010999015748031495,\n",
       " 0.9905262336735314,\n",
       " 0.0630603804797353,\n",
       " 9.8,\n",
       " 1.4,\n",
       " 0.9629703018861167]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in results:\n",
    "    temp = results[i]\n",
    "    TP = temp[0]\n",
    "    TN = temp[1]\n",
    "    FP = temp[2]\n",
    "    FN = temp[3]\n",
    "    auc_t= temp[4]\n",
    "    if i == 1:\n",
    "        accuracy = ((TP + TN) / float(TP + TN + FP + FN))\n",
    "        misclassication_rate = ((FP + FN) / float(TP + TN + FP + FN))\n",
    "        recall_tpr = (TP / float(TP + FN))\n",
    "        specificity = (TN / float(TN + FP))\n",
    "        fpr = (FP / float(TN + FP))\n",
    "        precision = (TP / float(TP + FP))\n",
    "        fnr = (FN / float(TP + FN))\n",
    "        false_negatives = FN\n",
    "        false_positives = FP\n",
    "        auc = auc_t\n",
    "    else:\n",
    "        accuracy += ((TP + TN) / float(TP + TN + FP + FN))\n",
    "        misclassication_rate += ((FP + FN) / float(TP + TN + FP + FN))\n",
    "        recall_tpr += (TP / float(TP + FN))\n",
    "        specificity += (TN / float(TN + FP))\n",
    "        fpr += (FP / float(TN + FP))\n",
    "        precision += (TP / float(TP + FP))\n",
    "        fnr += (FN / float(TP + FN))\n",
    "        false_negatives += FN\n",
    "        false_positives += FP\n",
    "        auc += auc_t\n",
    "res = [accuracy/5, misclassication_rate/5, recall_tpr/5, specificity/5, fpr/5,  precision/5, fnr/5, false_negatives/5, false_positives/5, auc/5 ]\n",
    "print('SVM Linear CV results:')\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of Nick's Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    " def conFusion_metricsOutput(var_y_test, var_y_pred, modname):\n",
    "    \n",
    "    res = []   \n",
    "    confusion = metrics.confusion_matrix(var_y_test, var_y_pred)\n",
    "    TP = confusion[1, 1] #True Positives (TP): we correctly predicted that tweets do have stigma\n",
    "    TN = confusion[0, 0] #True Negatives (TN): we correctly predicted that tweets don't have stigma\n",
    "    FP = confusion[0, 1] #False Positives (FP): we incorrectly predicted that tweets do have stigma (a \"Type I error\")\n",
    "    FN = confusion[1, 0] #False Negatives (FN): we incorrectly predicted that tweets don't have stigma (a \"Type II error\")\n",
    "\n",
    "    ## Classification Accuracy: Overall, how often is the classifier correct?\n",
    "    accuracy = ((TP + TN) / float(TP + TN + FP + FN))\n",
    "    #print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    #Classification Error: Overall, how often is the classifier incorrect?\n",
    "    #Also known as \"Misclassification Rate\"\n",
    "    misclassication_rate = ((FP + FN) / float(TP + TN + FP + FN))\n",
    "    #print(1 - metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    #Recall/Sensitivity: When the actual value is positive, how often is the prediction correct?\n",
    "    #How \"sensitive\" is the classifier to detecting positive instances?\n",
    "    #Also known as \"True Positive Rate\"\n",
    "    recall_tpr = (TP / float(TP + FN))\n",
    "    #print(metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "    #Specificity: When the actual value is negative, how often is the prediction correct?\n",
    "    #How \"specific\" (or \"selective\") is the classifier in predicting positive instances?\n",
    "    specificity = (TN / float(TN + FP))\n",
    "\n",
    "    #False Positive Rate: When the actual value is negative, how often is the prediction incorrect?\n",
    "    fpr = (FP / float(TN + FP))\n",
    "\n",
    "    #Precision: When a positive value is predicted, how often is the prediction correct?\n",
    "    #How \"precise\" is the classifier when predicting positive instances?\n",
    "    precision = (TP / float(TP + FP))\n",
    "    #print(metrics.precision_score(y_test, y_pred))\n",
    "    \n",
    "    fnr = (FN / float(TP + FN))\n",
    "    \n",
    "    false_negatives = FN\n",
    "    \n",
    "    false_positives = FP\n",
    "    \n",
    "    \n",
    "    res.append([accuracy, misclassication_rate, recall_tpr, specificity, fpr, precision, fnr, false_negatives, false_positives])\n",
    "    \n",
    "    data=pd.DataFrame(res ,columns=['accuracy','misclassication_rate', 'recall_tpr' , 'specificity' , 'fpr', 'precision', 'fnr', 'false_negatives', 'false_positives'], index=[modname]).T\n",
    "    print(TP, TN, FP, FN)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500, max_depth=25, n_jobs=-1, random_state=0)\n",
    "k_fold = KFold(n_splits=5)\n",
    "\n",
    "y_pred_rf_cv = cross_val_predict(rf, X_features_cv, df['stig_label'], cv=k_fold, n_jobs = -1)\n",
    "\n",
    "auc = roc_auc_score(df['stig_label'], y_pred_rf_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = conFusion_metricsOutput(df['stig_label'], y_pred_rf_cv, modname = 'rf_cv')\n",
    "res_df\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RF(n_est, depth):\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs=-1, random_state=0)\n",
    "    rf_model = rf.fit(X_train_vect, y_train)\n",
    "    y_pred = rf_model.predict(X_test_vect)\n",
    "    precision, recall, fscore, train_support = score(y_test, y_pred, pos_label=1, average=\"binary\")\n",
    "    print('Est: {} / Depth: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    n_est, depth, precision.mean(), recall.mean(),\n",
    "    (y_pred==y_test).sum() / len(y_pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extend if extremes show best\n",
    "%precision %.2f\n",
    "np.set_printoptions(precision=2)\n",
    "#200 estimators with 20 depth gives best results first time so redo up to 250\n",
    "\n",
    "for n_est in [10, 50, 100, 150, 200, 250]:\n",
    "    for depth in [10, 20, 30, 50, None]:\n",
    "        train_RF(n_est, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best is 200 est, depth 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf hold out test\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=20, n_jobs=-1, random_state=0) \n",
    "\n",
    "rf_model = rf.fit(X_train_vect, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test_vect)\n",
    "auc = roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = res_df.join(conFusion_metricsOutput(y_test, y_pred, 'rf_holdout'))\n",
    "\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=150, max_depth=None, random_state=0)\n",
    "k_fold = KFold(n_splits=5)\n",
    "\n",
    "y_pred_gb_cv = cross_val_predict(gb, X_features_cv, df['stig_label'], cv=k_fold, n_jobs = -1)\n",
    "\n",
    "auc = roc_auc_score(df['stig_label'], y_pred_gb_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = res_df.join(conFusion_metricsOutput(df['stig_label'], y_pred_gb_cv, modname = 'gb_cv'))\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GB(n_est, depth):\n",
    "    gb = GradientBoostingClassifier(n_estimators=n_est, max_depth=depth, random_state=0)\n",
    "    gb_model = gb.fit(X_train_vect, y_train)\n",
    "    y_pred = gb_model.predict(X_test_vect)\n",
    "    precision, recall, fscore, train_support = score(y_test, y_pred, pos_label=1, average=\"binary\")\n",
    "    print('Est: {} / Depth: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    n_est, depth, precision.mean(), recall.mean(),\n",
    "    (y_pred==y_test).sum() / len(y_pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_est in [10, 50, 100, 150, 200]:\n",
    "    for depth in [10, 20, 30, 50, None]:\n",
    "        train_GB(n_est, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best for GB is 10 estimators depth 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=10, max_depth=10, random_state=0)\n",
    "\n",
    "\n",
    "gb_model = gb.fit(X_train_vect, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = gb_model.predict(X_test_vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = res_df.join(conFusion_metricsOutput(y_test, y_pred, 'gb_holdout'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmClas = SVC(probability=True)\n",
    "\n",
    "y_pred_svm_cv = cross_val_predict(svmClas, X_features_cv, df['stig_label'], cv=k_fold, n_jobs = -1)\n",
    "\n",
    "auc = roc_auc_score(df['stig_label'], y_pred_svm_cv)\n",
    "\n",
    "res_df = res_df.join(conFusion_metricsOutput(df['stig_label'], y_pred_svm_cv, modname = 'svm_cv'))\n",
    "res_df\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_cost = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 100000, 1000000]\n",
    "\n",
    "for cst in SVM_cost:\n",
    "\n",
    "    svmClas = SVC(C = cst, random_state=0)\n",
    "\n",
    "    svmClas.fit(X_train_vect, y_train)\n",
    "\n",
    "    y_pred = svmClas.predict(X_test_vect)\n",
    "    precision, recall, fscore, train_support = score(y_test, y_pred, pos_label=1, average=\"binary\")\n",
    "    print('Cost function: {} --- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    cst, precision.mean(), recall.mean(),\n",
    "    (y_pred==y_test).sum() / len(y_pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function of 10000 is best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmClas = SVC(C = 10000, random_state=0)\n",
    "svmClas.fit(X_train_vect, y_train)\n",
    "\n",
    "pred = svmClas.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = res_df.join(conFusion_metricsOutput(y_test, pred, 'svm_holdout'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(y_test, pred)\n",
    "\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmClas = SVC(kernel='linear', probability=True)\n",
    "y_pred_svm_lin_cv = cross_val_predict(svmClas, X_features_cv, df['stig_label'], cv=k_fold, n_jobs = -1)\n",
    "\n",
    "auc = roc_auc_score(df['stig_label'], y_pred_svm_lin_cv)\n",
    "\n",
    "res_df = res_df.join(conFusion_metricsOutput(df['stig_label'], y_pred_svm_lin_cv, modname = 'svm_lin_cv'))\n",
    "res_df\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_cost = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "for cst in SVM_cost:\n",
    "\n",
    "    svmClas = SVC(kernel='linear', C = cst, random_state=0)\n",
    "\n",
    "    svmClas.fit(X_train_vect, y_train)\n",
    "    y_pred = svmClas.predict(X_test_vect)\n",
    "    precision, recall, fscore, train_support = score(y_test, y_pred, pos_label=1, average=\"binary\")\n",
    "    print('Cost function: {} --- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    cst, precision.mean(), recall.mean(),\n",
    "    (y_pred==y_test).sum() / len(y_pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best cost function for SVM linear is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmClas = SVC(kernel='linear', C = 1, random_state=0)\n",
    "svmClas.fit(X_train_vect, y_train)\n",
    "\n",
    "pred = svmClas.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = res_df.join(conFusion_metricsOutput(y_test, pred, 'svm_lin_holdout'))\n",
    "auc = roc_auc_score(y_test, pred)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# according to accuracy and false negatives, Random Forest holdout and SVM linear holdout are best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
