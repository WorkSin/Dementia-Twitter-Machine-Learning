{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "##Explore this blog: https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html for visualisation ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads rated tweets into dataframe\n",
    "df = pd.read_excel(\"C:/Users/ertur/Documents/Work/Workwork/ARUK/Submission - JMIR Aging/Revisions/Categorised tweets 1500.xlsx\", converters={'Tweet':str,'Theme':int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'Tweet':'body_text', 'Theme':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1497, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing cases where rating is missing\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining sentiment and subjectivity\n",
    "def sentAnal(df):\n",
    "    for index, row in df.iterrows():\n",
    "        temp = TextBlob(row['body_text'])\n",
    "        df.loc[index,'Sentiment'] = temp.sentiment.polarity\n",
    "        df.loc[index,'Subjectivity'] = temp.sentiment.subjectivity\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sentAnal(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1414, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing tweets rated as uncertain or unknown\n",
    "themes=[1,2,3,4,5,6]\n",
    "df = df[df.label.isin(themes)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting assigned themes into corresponding rating of stigmatising and non-stigmatising\n",
    "theme_map = {1:0, 2:0, 3:0, 4:1, 5:1, 6:1}\n",
    "df['stig_label'] = df.label.map(theme_map)\n",
    "df = df.drop('label', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#literature defined features are generated\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "df['body_len'] = df['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "df['punct%'] = df['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "# Average Word Length. simply take the sum of the length of all the words and divide it by the total length of the tweet as defined in function above\n",
    "df['avg_word'] = df['body_text'].apply(lambda x: avg_word(x))\n",
    "\n",
    "# Number of Words in tweet\n",
    "df['word_count'] = df['body_text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "# Number of characters. Here, we calculate the number of characters in each tweet. This is done by calculating the length of the tweet.\n",
    "df['char_count'] = df['body_text'].str.len() ## this also includes spaces\n",
    "\n",
    "# number of special characters like hashtags. we make use of the ‘starts with’ function because hashtags (or mentions) always appear at the beginning of a word.\n",
    "df['hastags'] = df['body_text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "\n",
    "# number of numerics in tweet\n",
    "df['numerics'] = df['body_text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "\n",
    "# number of UPPERCASE words. Anger or rage is quite often expressed by writing in UPPERCASE words which makes this a necessary operation to identify those words.\n",
    "df['upper'] = df['body_text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#care-partner defined features are generated\n",
    "#senile\n",
    "Search_for_These_values = ['senile', 'SENILE'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['senile'] = df['body_text'].str.contains(pattern)\n",
    "df['senile'] = df['senile'].map({True: 1, False: 0})\n",
    "#demented\n",
    "Search_for_These_values = ['demented', 'DEMENTED'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['demented'] = df['body_text'].str.contains(pattern)\n",
    "df['demented'] = df['demented'].map({True: 1, False: 0})\n",
    "#donald trump\n",
    "Search_for_These_values = ['donald', 'trump', 'DONALD', 'TRUMP', '@realDonaldTrump'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['donaldtrump'] = df['body_text'].str.contains(pattern)\n",
    "df['donaldtrump'] = df['donaldtrump'].map({True: 1, False: 0})\n",
    "#nancypelosi\n",
    "Search_for_These_values = ['nancy', 'pelosi', 'NANCY', 'PELOSI'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['nancypelosi'] = df['body_text'].str.contains(pattern)\n",
    "df['nancypelosi'] = df['nancypelosi'].map({True: 1, False: 0})\n",
    "#deranged\n",
    "Search_for_These_values = ['DERANGED', 'deranged'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['DERANGED'] = df['body_text'].str.contains(pattern)\n",
    "df['DERANGED'] = df['DERANGED'].map({True: 1, False: 0})\n",
    "#cafe\n",
    "Search_for_These_values = ['cafe', 'CAFE'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Cafe'] = df['body_text'].str.contains(pattern)\n",
    "df['Cafe'] = df['Cafe'].map({True: 1, False: 0})\n",
    "#insane\n",
    "Search_for_These_values = ['INSANE', 'insane'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Insane'] = df['body_text'].str.contains(pattern)\n",
    "df['Insane'] = df['Insane'].map({True: 1, False: 0})\n",
    "#memory\n",
    "Search_for_These_values = ['MEMORY', 'memory'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Memory'] = df['body_text'].str.contains(pattern)\n",
    "df['Memory'] = df['Memory'].map({True: 1, False: 0})\n",
    "#research\n",
    "Search_for_These_values = ['research', 'RESEARCH'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Research'] = df['body_text'].str.contains(pattern)\n",
    "df['Research'] = df['Research'].map({True: 1, False: 0})\n",
    "#imbecile\n",
    "Search_for_These_values = ['imbecile', 'IMBECILE'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Imbecile'] = df['body_text'].str.contains(pattern)\n",
    "df['Imbecile'] = df['Imbecile'].map({True: 1, False: 0})\n",
    "#loon\n",
    "Search_for_These_values = ['loon', 'LOON'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Loon'] = df['body_text'].str.contains(pattern)\n",
    "df['Loon'] = df['Loon'].map({True: 1, False: 0})\n",
    "#crazy\n",
    "Search_for_These_values = ['crazy', 'CRAZY'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Crazy'] = df['body_text'].str.contains(pattern)\n",
    "df['Crazy'] = df['Crazy'].map({True: 1, False: 0})\n",
    "#looney bin\n",
    "Search_for_These_values = ['looney', 'bin', 'LOONEY', 'BIN'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Looney_Bin'] = df['body_text'].str.contains(pattern)\n",
    "df['Looney_Bin'] = df['Looney_Bin'].map({True: 1, False: 0})\n",
    "#lunatic\n",
    "Search_for_These_values = ['lunatic', 'LUNATIC'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Lunatic'] = df['body_text'].str.contains(pattern)\n",
    "df['Lunatic'] = df['Lunatic'].map({True: 1, False: 0})\n",
    "#unhinged\n",
    "Search_for_These_values = ['unhinged', 'UNHINGED'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Unhinged'] = df['body_text'].str.contains(pattern)\n",
    "df['Unhinged'] = df['Unhinged'].map({True: 1, False: 0})\n",
    "#senility\n",
    "Search_for_These_values = ['senility', 'SENILITY'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Senility'] = df['body_text'].str.contains(pattern)\n",
    "df['Senility'] = df['Senility'].map({True: 1, False: 0})\n",
    "# URL\n",
    "Search_for_These_values = ['https'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Link'] = df['body_text'].str.contains(pattern)\n",
    "df['Link'] = df['Link'].map({True: 1, False: 0})\n",
    "#caregiver\n",
    "Search_for_These_values = ['caregiver', 'CAREGIVER'] \n",
    "pattern = '|'.join(Search_for_These_values) \n",
    "df['Caregiver'] = df['body_text'].str.contains(pattern)\n",
    "df['Caregiver'] = df['Caregiver'].map({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1414, 30)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df[df.columns.difference([\"stig_label\", \"body_text\"])].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1414, 17)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectFwe\n",
    "df_new = SelectFwe(alpha=0.05).fit_transform(df[cols], df['stig_label'])\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fixer = {}\n",
    "for number in range(17):\n",
    "    fixer[number] = []\n",
    "for row in df_new:\n",
    "    place = 0\n",
    "    for column in row: \n",
    "        fixer[place].append(float(column))\n",
    "        place += 1\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurechecker = df[df.columns.difference([\"stig_label\", \"body_text\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_features =[]\n",
    "for column in featurechecker:\n",
    "    iterable = [float(i) for i in featurechecker[column]]\n",
    "    samesofar = 0\n",
    "    for othercolumn in fixer:\n",
    "        if samesofar == 1414:\n",
    "            kept_features.append(column)\n",
    "            break\n",
    "        else:\n",
    "            for number in range(1414):\n",
    "                if iterable[number] == fixer[othercolumn][number]:\n",
    "                    samesofar += 1\n",
    "                    pass\n",
    "                else:\n",
    "                    samesofar = 0\n",
    "                    break\n",
    "            \n",
    "                \n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Caregiver',\n",
       " 'Crazy',\n",
       " 'Link',\n",
       " 'Memory',\n",
       " 'Research',\n",
       " 'Senility',\n",
       " 'Sentiment',\n",
       " 'avg_word',\n",
       " 'body_len',\n",
       " 'char_count',\n",
       " 'demented',\n",
       " 'donaldtrump',\n",
       " 'hastags',\n",
       " 'numerics',\n",
       " 'punct%',\n",
       " 'senile',\n",
       " 'body_text',\n",
       " 'stig_label']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "kept_features.append('body_text')\n",
    "kept_features.append('stig_label')\n",
    "kept_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
